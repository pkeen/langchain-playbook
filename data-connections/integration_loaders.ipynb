{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HackerNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=37852741\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimaxir 1 hour ago  \n",
      "[–] \n",
      "\n",
      "The discussion of \"memory storage\" is weird. You can't magically make ChatGPT remember more from before the context window, and I'm not sure how that would reduce costs.Perhaps it's a free vector store for retrieval-augmented generation, where instead from pulling from knowledge base vectors it pulls from previous conversation bullet point vectors?\n",
      " \n",
      "reply\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate.from_template(\"Please give me a short summary of the hacker news comment: {comment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt=PromptTemplate(input_variables=['comment'], template='Please give me a short summary of the hacker news comment: {comment}')\n"
     ]
    }
   ],
   "source": [
    "print(human_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['comment'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['comment'], template='Please give me a short summary of the hacker news comment: {comment}'))]\n"
     ]
    }
   ],
   "source": [
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[HumanMessage(content='Please give me a short summary of the hacker news comment: minimaxir 1 hour ago  \\n[–] \\n\\nThe discussion of \"memory storage\" is weird. You can\\'t magically make ChatGPT remember more from before the context window, and I\\'m not sure how that would reduce costs.Perhaps it\\'s a free vector store for retrieval-augmented generation, where instead from pulling from knowledge base vectors it pulls from previous conversation bullet point vectors?\\n \\nreply')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt.format_prompt(comment=data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(chat_prompt.format_prompt(comment=data[0].page_content).to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this Hacker News comment, the user minimaxir finds the discussion on \"memory storage\" to be strange. They explain that it is not possible for ChatGPT to remember more information beyond the context window. They also express uncertainty about how increasing memory storage would reduce costs. The user suggests that the feature might work as a free vector store for retrieval-augmented generation, where instead of pulling from knowledge base vectors, it pulls from previous conversation bullet point vectors.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-udemy-notebooks-Raz3oF7P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
